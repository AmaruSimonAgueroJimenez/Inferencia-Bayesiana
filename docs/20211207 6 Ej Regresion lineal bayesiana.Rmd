---
title: "Regresión Bayesiana"
output: 
---

```{r message=FALSE}
library(car)
```

```{r}
data("Leinhardt")
str(Leinhardt)
summary(Leinhardt)
```

Buscaremos modelar la mortalidad infantil en términos de otras variables.
Como el modelo que haremos no está optimizado, tendremos que sacar los datos faltantes antes de trabajar.

# Análisis descriptivos
```{r}
pairs(Leinhardt)
```

Vemos que entre ingresos e infancia no hay una relación de tipo lineal por lo que hay que aplicar una transformación para poder hacerlo. 

```{r}
plot(infant ~ income, data = Leinhardt)
hist(Leinhardt$infant)
hist(Leinhardt$income)
```

Pasamos las variables a escala logarítmica:
```{r}
Leinhardt$loginfant <- log(Leinhardt$infant)
Leinhardt$logincome <- log(Leinhardt$income)
```

```{r}
plot(loginfant ~ logincome, data=Leinhardt)
```

# Modelo 0: Regresión normal

```{r}
lmod <- lm(loginfant ~ logincome, data  =Leinhardt)
summary(lmod)
```

# Eliminamos datos faltantes
```{r}
dat <- na.omit(Leinhardt)
```

# Modelo 1: Regresión bayesiana
```{r message=FALSE}
library(rjags)
set.seed(123)
```



```{r}
mod1_string <- "model{

  for(i in 1:n){
    y[i] ~ dnorm(mu[i], prec)
    mu[i] = b[1] + b[2]*log_income[i]
  }

  for(j in 1:2){
    b[j] ~ dnorm(0.0, 1.0/1.0e6)
  }

  prec ~ dgamma(2.5, 25.0)
  sig2 = 1.0 /prec
  sig = sqrt(sig2)

}"

# Este bucle itera sobre todas las observaciones (n)
# Cada y[i] sigue una distribución normal con media mu[i] y precisión prec
# La media mu[i] se modela como una función lineal del logaritmo del ingreso
# b[1] es el intercepto y b[2] es la pendiente
# 
# for(j in 1:2){
#   b[j] ~ dnorm(0.0, 1.0/1.0e6)
# }
# Define priors no informativos para los coeficientes b[1] y b[2]
# Usa una distribución normal con media 0 y varianza muy grande (1e6)
# Este es un prior débil que permite que los datos dominen la inferencia
# 
# prec ~ dgamma(2.5, 25.0)
# sig2 = 1.0 /prec
# sig = sqrt(sig2)
# 
# La precisión (prec) sigue una distribución gamma con parámetros α=2.5 y β=25.0
# sig2 es la varianza (inversa de la precisión)
# sig es la desviación estándar
# Este prior para la precisión es algo informativo pero aún flexible

```

```{r}
data1_jags <- list(y = dat$loginfant,
                   n = nrow(dat),
                   log_income = dat$logincome)

params1 <- c("b", "sig")

# Crea una lista con los datos necesarios para el modelo JAGS
# y es la variable dependiente (logaritmo de mortalidad infantil)
# n es el número de observaciones
# log_income es la variable independiente (logaritmo del ingreso)
# params1 define los parámetros que queremos monitorear (coeficientes b y desviación estándar sig)
```

```{r}
# Define una función que genera valores iniciales aleatorios para el modelo
# Para b: genera 2 valores de una distribución normal(0, 100)
# Para prec: genera 1 valor de una distribución gamma(1, 1)
# Estos valores iniciales ayudan a iniciar las cadenas MCMC

inits1 <- function(){
  inits = list("b" = rnorm(2, 0.0, 100.0), #simulo 2 normales, por b1 y b2
               "prec" = rgamma(1, 1.0, 1.0)) 
}
```

```{r}
# Crea el modelo JAGS usando la especificación anterior
# Usa 3 cadenas MCMC independientes
# update(mod1, 1000) ejecuta 1000 iteraciones de "burn-in" (calentamiento)
# Estas iteraciones iniciales se descartan para eliminar la influencia de los valores iniciales

mod1 <- jags.model(textConnection(mod1_string), 
                   data = data1_jags,
                   inits = inits1,
                   n.chains = 3) #la idea es usar de 3 cadenas para arriba

update(mod1, 1)
```

```{r}
# Ejecuta la simulación MCMC para obtener muestras de la distribución posterior
# Realiza 5000 iteraciones para cada cadena
# Monitorea los parámetros especificados en params1 (b y sig)
# do.call(rbind, mod1_sim) combina las muestras de todas las cadenas en una única matriz

mod1_sim <- coda.samples(model = mod1,
                         variable.names = params1,
                         n.iter = 5e3)
mod1_csim <- do.call(rbind, mod1_sim)
```

##Convergencia
```{r}
library(coda)
```

```{r}
plot(mod1_sim)
```
las cadenas se ven con muy buen mixing, recorren bien el espacio de parámetros
las distribuciones originalmente eran centradas en 0 y ahora están desplazadas a otros valores
```{r}

# Esta función pertenece al paquete coda y calcula el diagnóstico de Gelman-Rubin para evaluar si las cadenas MCMC han convergido.
# Compara la variabilidad entre cadenas (varianza entre cadenas) con la variabilidad dentro de cada cadena (varianza dentro de cadenas).
# Devuelve un valor R
# R para cada parámetro monitoreado.
# R mide la relación entre la varianza total (entre y dentro de cadenas) y la varianza dentro de las cadenas.
# Valores R cercanos a 1 sugieren que las cadenas han convergido.
# R > 1.1 sugieren que las cadenas no han convergido completamente y que se necesitan más iteraciones.
gelman.diag(mod1_sim)
```
No hay evidencia de que el modelo no converja
```{r}
# Devuelve una tabla o matriz con los valores de autocorrelación para cada parámetro monitoreado, en diferentes retardos (lags).
# Los retardos (lags) representan la distancia entre las muestras que se están comparando. Por ejemplo:
# Lag 0: Autocorrelación de una muestra consigo misma (siempre es 1).
# Lag 1: Autocorrelación entre una muestra y la siguiente.
# Lag 2: Autocorrelación entre una muestra y la segunda siguiente, y así sucesivamente.
# Si los valores de autocorrelación disminuyen rápidamente hacia 0, las muestras son menos dependientes entre sí, lo cual es deseable.
# Si los valores de autocorrelación permanecen altos incluso en lags grandes, significa que las cadenas tienen alta dependencia entre muestras consecutivas. Esto puede reducir la eficiencia del muestreo.
autocorr(mod1_sim)
```

```{r}
# Calcula el tamaño efectivo de muestra (Effective Sample Size, ESS) para cada parámetro monitoreado.
# El ESS estima cuántas muestras independientes equivalentes hay en las cadenas MCMC, teniendo en cuenta la autocorrelación.
# Aunque generes muchas iteraciones, si las cadenas tienen alta autocorrelación, el ESS será mucho menor que el número total de muestras.
# Salida
# Devuelve un valor para cada parámetro, que representa el tamaño efectivo de muestra.
# Ejemplo de salida
# Si mod1_sim contiene las cadenas para los parámetros b[1], b[2] y sig, la salida podría ser algo como esto:
# 
# b[1]   1500
# b[2]   1200
# sig     800
# Interpretación
# Valores altos: Un ESS alto (cercano al número total de iteraciones) indica que las muestras son casi independientes y el muestreo es eficiente.
# Valores bajos: Un ESS bajo indica que las cadenas tienen alta autocorrelación, lo que reduce la cantidad de información útil en las muestras.
# Regla general: Un ESS menor a 100 puede ser problemático, ya que indica que las cadenas no están proporcionando suficientes muestras independientes.

effectiveSize(mod1_sim)
```
tengo mucho menos información que lo que simulé
hay mucha dependencia entre los datos
si hubiésemos simulado una cadena muy larga, valdría la pena quitar valores

##Resultados y residuos
```{r}
summary(mod1_sim)
summary(lmod)
intervalo.hpd <- HPDinterval(as.mcmc(mod1_csim), prob = .95)
intervalo.hpd
```

Analicemos residuos que sé que van a funcionar mal, como los del modelo sin haber usado la regresión logarítmica
```{r}
lmod0 = lm(infant ~ income, data = Leinhardt)
plot(resid(lmod0))
```
Esto sirve para ver:
- si son demasiado grandes (acá está muy grande)
- la independencia entre los residuos
Son malos

Otra forma de verlo:
```{r}
plot(predict(lmod0), resid(lmod0))
qqnorm(resid(lmod0)); qqline(resid(lmod0))
library(nortest)
lillie.test(resid(lmod0))
```
Cuando quiero hacer los residuos bayesianos, uso los estimadores de Bayes
```{r}
X <- cbind(rep(1, data1_jags$n,), data1_jags$log_income)
pm_params1 <- colMeans(mod1_csim)
yhat1 <- drop(X %*% pm_params1[1:2]) #esta es la predicción
resid1 <- data1_jags$y - yhat1 #residuos: datos - predicción
plot(resid1)
```
Los errores que da están bastante mejor, aunque igual tiene unos valores grandes.

```{r}
plot(yhat1, resid1)
qqnorm(resid1); qqline(resid1)
lillie.test(resid1)
```

```{r}
head(rownames(dat)[order(resid1, decreasing = T)])
```

"Saudi.Arabia" y "Libya" exportan petróleo, introduciremos la covariable "oil" en el análisis para ver si es efecto de eso

#Modelo 2

```{r}
mod2_string <- "model{
  for(i in 1:n){
    y[i] ~ dnorm(mu[i], prec)
    mu[i] = b[1] + b[2]*log_income[i] + b[3]*is_oil[i]
  }
  for(j in 1:3){
    b[j] ~ dnorm(0.0, 1.0/1.0e6)
  }
  prec ~ dgamma(2.5, 25.0)
  sig2 = 1.0 /prec
  sig = sqrt(sig2)
}"
```

```{r}
data2_jags <- list(y = dat$loginfant,
                   n = nrow(dat),
                   log_income = dat$logincome,
                   is_oil = as.numeric(dat$oil=="yes"))

params2 <- c("b", "sig")
```

```{r}
inits2 <- function(){
  inits = list("b" = rnorm(3, 0.0, 100.0), #simulo 3 normales, por b1, b2 y b3
               "prec" = rgamma(1, 1.0, 1.0)) 
}
```

```{r}
mod2 <- jags.model(textConnection(mod2_string), 
                   data = data2_jags,
                   inits = inits2,
                   n.chains = 3) #la idea es usar de 3 cadenas para arriba

update(mod2, 1000)
```

```{r}
mod2_sim <- coda.samples(model = mod2,
                         variable.names = params2,
                         n.iter = 5e3)
mod2_csim <- do.call(rbind, mod2_sim)
```

##Convergencia
```{r}
plot(mod2_sim)
gelman.diag(mod2_sim)
autocorr(mod2_sim)
effectiveSize(mod2_sim)
```

##Resultados y residuos
```{r}
summary(mod2_sim)
intervalo.hpd <- HPDinterval(as.mcmc(mod2_csim), prob = .95)
intervalo.hpd
```

```{r}
X2 <- cbind(rep(1, data2_jags$n,), data2_jags$log_income, data2_jags$is_oil)
pm_params2 <- colMeans(mod2_csim) #parámetros a posteriori
yhat2 <- drop(X2 %*% pm_params2[1:3]) #esta es la predicción
resid2 <- data2_jags$y - yhat2 #residuos: datos - predicción
plot(resid2)
plot(yhat2, resid2)
qqnorm(resid2); qqline(resid2)
lillie.test(resid2)
```

```{r}
par(mfrow = c(2,1))
plot(yhat2, resid2)
plot(yhat1, resid1)
```

Logré bajar los residuos de los outliers


#Modelo 3

```{r}
par(mfrow = c(1,1))
curve(dnorm(x), from = -5, to = 5)
curve(dt(x,1), from = -5, to = 5, col="red", add=T))
```
Una t tiene colas mucho más pesadas que la normal
¿Cómo ajusto una normal a través de una t, para poder controlar la varianza?
Quiero que los errores tengan colas más grandes, no los coeficientes, así que de ellos puedo mantener la normalidad
```{r}
mod3_string <- "model{
  for(i in 1:n){
    y[i] ~ dt(mu[i], tau, df)
    mu[i] = b[1] + b[2]*log_income[i] + b[3]*is_oil[i]
  }
  for(j in 1:3){
    b[j] ~ dnorm(0.0, 1.0/1.0e6)
  }
  df = 2.0 + nu
  nu ~ dexp(1.0)
  tau ~ dgamma(2.5, 25.0)
  sig = sqrt(1.0 / tau * df / (df - 2.0))
}"
```


SEGUIR COMPLETANDO DESDE ACÁ COMO ANTES
hay que actualizar nu simulando una exponencial